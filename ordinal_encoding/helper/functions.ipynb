{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import sklearn.linear_model as sk\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, log_loss\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-Implemented Training Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_vs_all(X, y, interested_in, num_classes, lambda_val):\n",
    "    \"\"\"\n",
    "    Train a one vs. all logistic regression\n",
    "    \n",
    "    :param X: The input matrix (ndarray)\n",
    "    :param y: Label vector (ndarray)\n",
    "    :param num_classes: Number of classes (int)\n",
    "    :param lambda_val: Regularization parameter (float)\n",
    "\n",
    "    :return weight_vectors: Weight vector matrix (ndarray)\n",
    "    :return intercepts: Intercept vector matrix (ndarray)                     \n",
    "            \n",
    "    \"\"\"\n",
    "    weight_vectors = np.zeros((X.shape[1], num_classes))\n",
    "    intercepts = np.zeros(num_classes) \n",
    "\n",
    "    for i in range(len(interested_in)):\n",
    "        if interested_in[i] in y:\n",
    "            y_c = (y == interested_in[i]).astype(int)\n",
    "            weight_vectors[:, i], intercepts[i] = train_logistic_regression(X, y_c, lambda_val)\n",
    "\n",
    "    return weight_vectors, intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_vs_all(X, weight_vectors, intercepts):\n",
    "    \"\"\"\n",
    "    Predict one vs. all logistic regression\n",
    "    \n",
    "    :param X: The input matrix (ndarray)\n",
    "    :param weight_vectors: Weight vector matrix (ndarray)\n",
    "    :param intercepts: Intercept vector matrix (ndarray)     \n",
    "                       \n",
    "    :return predictions: Prediction vector (ndarray) \n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    predictions = np.argmax(np.add(np.dot(X,weight_vectors),intercepts), 1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X, y, lambda_val):\n",
    "    \"\"\"\n",
    "    Train a regularized logistic regression model\n",
    "    \n",
    "    :param X: The input matrix (ndarray)\n",
    "    :param y: Label vector (ndarray)\n",
    "    :param lambda_val: Regularization parameter (float)\n",
    "\n",
    "    :return weights: Weight vector (ndarray)\n",
    "    :return intercept: Intercept parameter (float)   \n",
    "    \n",
    "    \"\"\"\n",
    "    model = linear_model.LogisticRegression(C=2./lambda_val, solver='lbfgs')\n",
    "\n",
    "    # call model.fit(X, y) while suppressing warnings about convergence\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        model.fit(X, y)\n",
    "\n",
    "    weight_vector = model.coef_.ravel()\n",
    "    intercept = model.intercept_\n",
    "    return weight_vector, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    \"\"\"\n",
    "    The logistic function\n",
    "    \n",
    "    :param z: Array to be converted (ndarray)\n",
    "    \n",
    "    :return p: logistic(z) entrywise of the same shape (ndarray)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ones_z = np.ones(z.shape)\n",
    "    exp = np.full(z.shape, np.e)\n",
    "    denum = ones_z.copy() + np.power(exp, -z)\n",
    "    p = np.divide(ones_z, denum)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_cost_function(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the negative log liklihood (nll) cost function for a particular data \n",
    "    set and hypothesis (weight vector)\n",
    "    \n",
    "    :param X: The input matrix (ndarray)\n",
    "    :param y: Label vector (ndarray)\n",
    "    :param theta: Initial parameter vector (ndarray)\n",
    "    \n",
    "    :return cost: The value of the cost function (float)\n",
    "    \n",
    "    \"\"\"\n",
    "    cost = 0\n",
    "    h = logistic(np.dot(X,theta)).astype('int')\n",
    "    cost = -np.dot(y, np.log(h)) - np.dot((1-y), np.log(1 - h))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, iters):\n",
    "    \"\"\"\n",
    "    Fits a logistic regression model by gradient descent.\n",
    "    \n",
    "    :param X: The input matrix (ndarray)\n",
    "    :param y: Label vector (ndarray)\n",
    "    :param theta: Initial parameter vector (ndarray)\n",
    "    :param alpha: Step size (float)\n",
    "    :param iters: Number of iterations (int)\n",
    "    \n",
    "    :return theta: Learned parameter vector (ndarray)\n",
    "    :return J_history: Cost function in iteration (ndarray)\n",
    "        \n",
    "    \"\"\"\n",
    "    J_history = np.zeros(iters)\n",
    "\n",
    "    for i in range(iters):\n",
    "        d_J = 2*(np.dot(X.T, np.subtract(logistic(np.dot(X,theta)),y)))\n",
    "        theta = theta - (alpha*d_J)\n",
    "        J_history[i] = nll_cost_function(X, y, theta)\n",
    "        \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vital status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vital_status_train(X, y, alpha, iters, num_groups, feature_labels, num_tests = 1, output = True):\n",
    "        \n",
    "    if output == True:\n",
    "        for i in range(num_tests):    \n",
    "            X_groups, y_groups = get_vital_status_data(X, y, num_groups)\n",
    "\n",
    "            print(\"In class model:\")\n",
    "            vital_status_in_class(X_groups, y_groups, alpha, iters, num_groups)\n",
    "            print()\n",
    "            print(\"Scikit model:\")\n",
    "            vital_status_scikit(X_groups, y_groups, num_groups, feature_labels)\n",
    "    else:\n",
    "        accuracy_class = 0\n",
    "        accuracy_scikit = 0\n",
    "        \n",
    "        for i in range(num_tests):    \n",
    "            X_groups, y_groups = get_vital_status_data(X, y, num_groups)\n",
    "            accuracy_class = accuracy_class + vital_status_in_class(X_groups, y_groups, alpha, iters, num_groups, output)\n",
    "            accuracy_scikit = accuracy_scikit + vital_status_scikit(X_groups, y_groups, num_groups, feature_labels, output)\n",
    "        print(\"In class model:\")\n",
    "        print(\"Average test accuracy: %.2f\" % (accuracy_class/num_tests),\"%\")\n",
    "        print(\"Scikit model:\")\n",
    "        print(\"Average test accuracy: %.2f\" % (accuracy_scikit/num_tests),\"%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation groups for vital_status\n",
    "def get_vital_status_data(X, y, num_groups):  \n",
    "    # Equate the number of dead/alive cases\n",
    "    dead = np.nonzero(y == 0)[0]\n",
    "    alive = np.nonzero(y == 1)[0]\n",
    "    np.random.shuffle(dead)\n",
    "    np.random.shuffle(alive)\n",
    "    num_cases = min(len(dead), len(alive))\n",
    "    dead = dead[0:num_cases]\n",
    "    alive = alive[0:num_cases]\n",
    "    \n",
    "    # Form the balanced cross-validation subsets (indices)\n",
    "    dead_groups = np.array_split(dead, num_groups)\n",
    "    alive_groups = np.array_split(alive, num_groups)\n",
    "\n",
    "    subsets = []\n",
    "    for i in range(num_groups):\n",
    "        assert(len(dead_groups[i]) == len(alive_groups[i]))\n",
    "        subsets.append(np.concatenate((dead_groups[i], alive_groups[i])))\n",
    "        np.random.shuffle(subsets[i])\n",
    "\n",
    "    # Form the balanced cross-validation X and y (values)\n",
    "    X_groups = []\n",
    "    y_groups = []\n",
    "    for i in range(num_groups):\n",
    "        X_groups.append(X[subsets[i],:])\n",
    "        y_groups.append(y[subsets[i]])\n",
    "    return X_groups, y_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vital_status_in_class(X_groups, y_groups, alpha, iters, num_groups, output = True):\n",
    "    # Define params for the training\n",
    "    n = X_groups[0].shape[1]\n",
    "    theta = np.ones(n)\n",
    "\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    train_f1_score = 0\n",
    "    test_f1_score = 0\n",
    "    cv_cost = 0\n",
    "    test_confusion = np.zeros(shape=(2, 2))\n",
    "\n",
    "    # Perform k-fold cross-validation and record the results\n",
    "    for i in range(num_groups):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        X_train, y_train = get_train_groups(X_groups, y_groups, num_groups, i)\n",
    "\n",
    "        theta, J = gradient_descent(X_train, y_train, theta, alpha, iters)\n",
    "        cv_cost = cv_cost + J[-1]\n",
    "\n",
    "        sort_predictions = lambda x: 0 if x < 0.5 else 1\n",
    "        convert_to_log = np.vectorize(sort_predictions)\n",
    "        \n",
    "        # Apply sigmoid to the test predictions\n",
    "        train_preds = logistic(np.dot(X_train, theta))\n",
    "        test_preds = logistic(np.dot(X_test, theta))\n",
    "        train_preds = convert_to_log(train_preds)\n",
    "        test_preds = convert_to_log(test_preds)\n",
    "        \n",
    "        accuracy, f1 = get_stats(y_train, train_preds)\n",
    "        train_accuracy = train_accuracy + accuracy\n",
    "        train_f1_score = train_f1_score + f1\n",
    "        \n",
    "        accuracy, f1 = get_stats(y_test, test_preds)\n",
    "        test_accuracy = test_accuracy + accuracy\n",
    "        test_f1_score = test_f1_score + f1\n",
    "        test_confusion = test_confusion + confusion_matrix(y_test, test_preds)\n",
    "    if output:\n",
    "        print(\"Average train final cost: %.2f\" % (cv_cost/num_groups))\n",
    "        print(\"Average train accuracy: %.2f\" % (train_accuracy/num_groups),\"%\")\n",
    "        print(\"Average train F1 score: %.2f\" % (train_f1_score/num_groups))\n",
    "\n",
    "        print(\"Average test accuracy: %.2f\" % (test_accuracy/num_groups),\"%\")\n",
    "        print(\"Average test F1 score: %.2f\" % (test_f1_score/num_groups))\n",
    "        create_heatmap((test_confusion/num_groups).astype('int'), [0, 1])\n",
    "    return test_accuracy/num_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vital_status_scikit(X_groups, y_groups, num_groups, feature_labels,  output = True):\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    train_f1_score = 0\n",
    "    test_f1_score = 0\n",
    "    train_cost = 0\n",
    "    test_cost = 0\n",
    "    test_weights = np.zeros(X_groups[0].shape[1])\n",
    "    test_confusion = np.zeros(shape=(2, 2)) \n",
    "    test_bayes = 0\n",
    "\n",
    "    # Perform k-fold cross-validation and record the results \n",
    "    for i in range(num_groups):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i] \n",
    "        X_train, y_train = get_train_groups(X_groups, y_groups, num_groups, i)\n",
    "\n",
    "        lr = sk.LogisticRegression(solver='liblinear')\n",
    "        lr.fit(X_train, y_train)\n",
    "        train_preds = lr.predict(X_train)\n",
    "        test_preds = lr.predict(X_test)\n",
    "        \n",
    "        gnb = GaussianNB()\n",
    "        gnb_preds = gnb.fit(X_train, y_train).predict(X_test)\n",
    "        test_bayes = test_bayes + accuracy_score(y_test, gnb_preds)*100\n",
    "        \n",
    "        train_accuracy = train_accuracy + accuracy_score(y_train, train_preds)*100\n",
    "        train_f1_score = train_f1_score + f1_score(y_train, train_preds)\n",
    "        train_cost = train_cost + log_loss(y_train, train_preds)\n",
    "        \n",
    "        test_accuracy = test_accuracy + accuracy_score(y_test, test_preds)*100\n",
    "        test_f1_score = test_f1_score + f1_score(y_test, test_preds)\n",
    "        test_cost = test_cost + log_loss(y_test, test_preds)\n",
    "        \n",
    "        test_weights = np.add(test_weights, lr.coef_[0])\n",
    "        test_confusion = test_confusion + confusion_matrix(y_test, test_preds)\n",
    "\n",
    "#     UNCOMMENT to access uncompressed weights\n",
    "#     test_weights_abs = list(np.abs(test_weights))\n",
    "#     test_weights_sorted = test_weights_abs.copy()\n",
    "#     test_weights_sorted.sort(reverse = True)\n",
    "\n",
    "    actual_labels, actual_weights = get_actual_weights(feature_labels, list(test_weights))    \n",
    "    test_weights_abs = list(np.abs(actual_weights))\n",
    "    test_weights_sorted = test_weights_abs.copy()\n",
    "    test_weights_sorted.sort(reverse = True)\n",
    "    \n",
    "    if output:\n",
    "        print(\"Average train final cost: %.2f\" % (train_cost/num_groups))\n",
    "        print(\"Average train accuracy: %.2f\" % (train_accuracy/num_groups),\"%\")\n",
    "        print(\"Average train F1 score: %.2f\" % (train_f1_score/num_groups))\n",
    "        print(\"Average test final cost: %.2f\" % (test_cost/num_groups))\n",
    "        print(\"Average test accuracy: %.2f\" % (test_accuracy/num_groups),\"%\")\n",
    "        print(\"Average test F1 score: %.2f\" % (test_f1_score/num_groups))\n",
    "        print(\"Average Gaussian Naive Bayes accuracy: %.2f\" % (test_bayes/num_groups),\"%\")\n",
    "        create_heatmap((test_confusion/num_groups).astype('int'), [0, 1])\n",
    "        print()\n",
    "        print(\"Highest average weights (absolute values!):\")\n",
    "        for i in range(len(test_weights_sorted[:3])):\n",
    "            index = test_weights_abs.index(test_weights_sorted[i])\n",
    "            print(actual_labels[index], ': %.2f' % test_weights_sorted[i])\n",
    "    return test_accuracy/num_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Death days to**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def death_days_to_train(X, y, alpha, iters, num_groups):\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    final_cost = 0\n",
    "    \n",
    "    # Get cross-validation grouped X and y data\n",
    "    X_groups, y_groups = get_death_days_to_data(X, y, num_groups)\n",
    "\n",
    "    # Perform k-fold cross-validation and record the results \n",
    "    for i in range(num_groups):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        X_train, y_train = get_train_groups(X_groups, y_groups, num_groups, i)\n",
    "\n",
    "        # Scikit model training\n",
    "        lr = sk.LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        train_preds = lr.predict(X_train)\n",
    "        test_preds = lr.predict(X_test)\n",
    "\n",
    "        # In-class, only cost\n",
    "        theta_vec_equations, cost = gradient_descent_v2(normalize_v2(X_train), y_train, alpha, iters)   \n",
    "        final_cost = final_cost + cost[-1]\n",
    "\n",
    "        # Calculate the train accuracy\n",
    "        match = 0\n",
    "        for i in range(len(y_train)):\n",
    "            if  train_preds[i] <= y_train[i] + 180 and train_preds[i] >= y_train[i] - 180:\n",
    "                match += 1\n",
    "        train_accuracy = train_accuracy + match/len(y_train)*100 \n",
    "\n",
    "        # Calculate the test accuracy\n",
    "        match = 0\n",
    "        for i in range(len(y_test)):\n",
    "            if  test_preds[i] <= y_test[i] + 180 and test_preds[i] >= y_test[i] - 180:\n",
    "                match += 1\n",
    "        test_accuracy = test_accuracy + match/len(y_test)*100 \n",
    "\n",
    "    print(\"In-class model:\")\n",
    "    print(\"Average final cost: %.2f\" % (final_cost/num_groups))\n",
    "    print(\"Mean in y: %.2f\" % np.mean(y))\n",
    "    print(\"Variance in y: %.2f\" % np.var(y))\n",
    "    print()\n",
    "    print(\"Scikit model:\")\n",
    "    print(\"Average train accuracy: %.2f\" % (train_accuracy/num_groups),\"%\")\n",
    "    print(\"Average test accuracy: %.2f\" % (test_accuracy/num_groups),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation groups for death_days_to\n",
    "def get_death_days_to_data(X, y, num_groups):    \n",
    "    # Get all dead cases\n",
    "    death_indices = np.arange(len(y))\n",
    "    np.random.shuffle(death_indices)\n",
    "    \n",
    "    dead_groups = np.array_split(death_indices, num_groups)\n",
    "\n",
    "    X_groups = []\n",
    "    y_groups = []\n",
    "    for i in range(num_groups):\n",
    "        X_groups.append(X[dead_groups[i],:])\n",
    "        y_groups.append(y[dead_groups[i]])\n",
    "    return X_groups, y_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_train(X, y, interested_in, lambda_val, num_groups, feature_labels, alpha = 0.00000001, iters = 2000, num_tests = 1, output = True):\n",
    "    if output == True:\n",
    "        for i in range(num_tests):    \n",
    "            X_groups, y_groups, num_classes = get_outcome_data(X, y, interested_in, num_groups)\n",
    "\n",
    "            print(\"In class model:\")\n",
    "            if num_classes == 2:\n",
    "                for i in range(len(y_groups)):\n",
    "                    y_groups[i] = np.where(y_groups[i] == interested_in[0], 0, 1)                    \n",
    "                print(\"Logistic regression:\")\n",
    "                vital_status_in_class(X_groups, y_groups, alpha, iters, num_groups)\n",
    "                print()\n",
    "\n",
    "            print(\"One vs all:\")  \n",
    "            outcome_in_class(X_groups, y_groups, num_classes, interested_in, lambda_val, num_groups)\n",
    "\n",
    "            print()\n",
    "            print(\"Scikit model:\")\n",
    "            outcome_scikit(X_groups, y_groups, num_classes, interested_in, num_groups, feature_labels)\n",
    "            plot_lambdas(X_groups, y_groups, num_classes, interested_in, num_groups)\n",
    "    else:\n",
    "        accuracy_regression = 0\n",
    "        accuracy_classifier = 0\n",
    "        accuracy_scikit = 0\n",
    "        \n",
    "        for i in range(num_tests):  \n",
    "            X_groups, y_groups, num_classes = get_outcome_data(X, y, interested_in, num_groups)\n",
    "\n",
    "            if num_classes == 2:\n",
    "                for i in range(len(y_groups)):\n",
    "                    y_groups[i] = np.where(y_groups[i] == interested_in[0], 0, 1)                    \n",
    "                accuracy_regression = accuracy_regression + vital_status_in_class(X_groups, y_groups, alpha, iters, num_groups, output) \n",
    "            accuracy_classifier = accuracy_classifier + outcome_in_class(X_groups, y_groups, num_classes, interested_in, lambda_val, num_groups, output)\n",
    "            accuracy_scikit = accuracy_scikit + outcome_scikit(X_groups, y_groups, num_classes, interested_in, num_groups, feature_labels, output)\n",
    "                    \n",
    "        if num_classes == 2:\n",
    "            print(\"Logistic regression:\")\n",
    "            print(\"Average test accuracy: %.2f\" % (accuracy_regression/num_tests),\"%\")\n",
    "        print(\"One vs all model:\")\n",
    "        print(\"Average test accuracy: %.2f\" % (accuracy_classifier/num_tests),\"%\")\n",
    "        print(\"Scikit model:\")\n",
    "        print(\"Average test accuracy: %.2f\" % (accuracy_scikit/num_tests),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcome_data(X, y, interested_in, num_groups):\n",
    "    # Get the number of samples to take from each class\n",
    "    lengths = []\n",
    "    data = []\n",
    "    for i in range(5): \n",
    "        if i in interested_in:\n",
    "            data.append(np.nonzero(y == i)[0])\n",
    "            lengths.append(len(data[-1]))    \n",
    "    num_cases = np.amin(lengths)\n",
    "    num_classes = len(lengths)\n",
    "    \n",
    "    \n",
    "    # Truncate to balance\n",
    "    for i in range(num_classes):\n",
    "        np.random.shuffle(data[i])\n",
    "        data[i] = data[i][0:num_cases]\n",
    "    \n",
    "    # Break into balanced groups\n",
    "    batch_groups = []\n",
    "    for i in range(num_classes):\n",
    "        batch_groups.append(np.array_split(data[i], num_groups))\n",
    "\n",
    "    # Put batches together \n",
    "    outcome_groups = []\n",
    "    for i in range(num_groups): #0-4\n",
    "        batch = []\n",
    "        for j in range(num_classes): #0-2\n",
    "            batch = np.concatenate([batch, batch_groups[j][i]])\n",
    "        np.random.shuffle(batch)\n",
    "        outcome_groups.append(batch.astype('int'))\n",
    "    \n",
    "    X_groups = []\n",
    "    y_groups = []\n",
    "    for i in range(num_groups):\n",
    "        X_groups.append(X[outcome_groups[i],:])\n",
    "        y_groups.append(y[outcome_groups[i]])\n",
    "    return X_groups, y_groups, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug 1.1: confusion matrix is not exactly what we need it to be. \n",
    "# Bug 1.2: test_confusion is not initialized right\n",
    "\n",
    "def outcome_in_class(X_groups, y_groups, num_classes, interested_in, lambda_val, num_groups, output = True):\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    train_f1_score = 0\n",
    "    test_f1_score = 0    \n",
    "    test_confusion = np.zeros(shape=(len(interested_in), len(interested_in))) \n",
    "    \n",
    "    target_names = []\n",
    "    for i in interested_in:\n",
    "        target_names.append('class ' + str(i))\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        X_train, y_train = get_train_groups(X_groups, y_groups, num_groups, i)\n",
    "        \n",
    "        weight_vectors, intercepts = train_one_vs_all(X_train, y_train, interested_in, num_classes, lambda_val)\n",
    "        train_preds = predict_one_vs_all(X_train, weight_vectors, intercepts)\n",
    "        test_preds  = predict_one_vs_all(X_test,  weight_vectors, intercepts)\n",
    "\n",
    "        accuracy, f1 = get_stats(y_train, train_preds)\n",
    "        train_accuracy = train_accuracy + accuracy\n",
    "        train_f1_score = train_f1_score + f1\n",
    "        \n",
    "        accuracy, f1 = get_stats(y_test, test_preds)\n",
    "        test_accuracy = test_accuracy + accuracy\n",
    "        test_f1_score = test_f1_score + f1\n",
    "        test_confusion = test_confusion + confusion_matrix(list(y_test), list(test_preds))\n",
    "    if output:\n",
    "        print(\"Average train accuracy: %.2f\" % (train_accuracy/num_groups),\"%\")\n",
    "        print(\"Average train F1 score: %.2f\" % (train_f1_score/num_groups))\n",
    "\n",
    "        print(\"Average test accuracy: %.2f\" % (test_accuracy/num_groups),\"%\")\n",
    "        print(\"Average test F1 score: %.2f\" % (test_f1_score/num_groups))   \n",
    "        create_heatmap((test_confusion/num_groups).astype('int'), interested_in)\n",
    "    return test_accuracy/num_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_scikit(X_groups, y_groups, num_classes, interested_in, num_groups, feature_labels, output = True):\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    train_f1_score = 0\n",
    "    test_f1_score = 0\n",
    "    test_weights = np.zeros(X_groups[0].shape[1])\n",
    "    test_confusion = np.zeros(shape=(len(interested_in), len(interested_in)))\n",
    "    test_bayes = 0\n",
    "    \n",
    "    target_names = []\n",
    "    for i in interested_in:\n",
    "        target_names.append('class ' + str(i))\n",
    "    \n",
    "    for i in range(num_groups):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        X_train, y_train = get_train_groups(X_groups, y_groups, num_groups, i)\n",
    "\n",
    "        lr = sk.LogisticRegression()\n",
    "        lr.fit(X_train, y_train.astype('int'))\n",
    "        train_preds = lr.predict(X_train)\n",
    "        test_preds = lr.predict(X_test)\n",
    "        \n",
    "        gnb = GaussianNB()\n",
    "        gnb_preds = gnb.fit(X_train, y_train.astype('int')).predict(X_test)\n",
    "        test_bayes = test_bayes + accuracy_score(list(y_test), list(gnb_preds))*100\n",
    "\n",
    "        train_accuracy = train_accuracy + np.mean(train_preds == y_train)*100\n",
    "        test_accuracy = test_accuracy + np.mean(test_preds == y_test)*100 \n",
    "        train_class_report = classification_report(list(y_train), list(train_preds), \\\n",
    "                                              target_names = target_names, output_dict=True)\n",
    "        test_class_report = classification_report(list(y_test), list(test_preds), \\\n",
    "                                              target_names = target_names, output_dict=True)\n",
    "        # Calculate average test f1-score\n",
    "        train_f1_acc = 0\n",
    "        test_f1_acc = 0\n",
    "        for name in target_names:\n",
    "            train_f1_acc = train_f1_acc + train_class_report[name]['f1-score']/len(target_names)\n",
    "            test_f1_acc = test_f1_acc + test_class_report[name]['f1-score']/len(target_names)\n",
    "        train_f1_score = train_f1_score + train_f1_acc\n",
    "        test_f1_score = test_f1_score + test_f1_acc\n",
    "        test_weights = np.add(test_weights, lr.coef_[0])\n",
    "        test_confusion = test_confusion + confusion_matrix(list(y_test), list(test_preds))\n",
    "    \n",
    "#     UNCOMMENT to access uncompressed weights\n",
    "#     test_weights_abs = list(np.abs(test_weights))\n",
    "#     test_weights_sorted = test_weights_abs.copy()\n",
    "#     test_weights_sorted.sort(reverse = True)\n",
    "\n",
    "    actual_labels, actual_weights = get_actual_weights(feature_labels, list(test_weights))   \n",
    "    test_weights_abs = list(np.abs(actual_weights))\n",
    "    test_weights_sorted = test_weights_abs.copy()\n",
    "    test_weights_sorted.sort(reverse = True)\n",
    "    \n",
    "    if output:\n",
    "        print(\"Average train accuracy: %.2f\" % (train_accuracy/num_groups),\"%\")\n",
    "        print(\"Average train F1 score: %.2f\" % (train_f1_score/num_groups))\n",
    "        print(\"Average test accuracy: %.2f\" % (test_accuracy/num_groups),\"%\")\n",
    "        print(\"Average test F1 score: %.2f\" % (test_f1_score/num_groups))\n",
    "        print(\"Average Gaussian Naive Bayes accuracy: %.2f\" % (test_bayes/num_groups),\"%\")\n",
    "        create_heatmap((test_confusion/num_groups).astype('int'), interested_in)\n",
    "        print()\n",
    "        print(\"Highest average weights (absolute values!):\")\n",
    "        for i in range(len(test_weights_sorted[:3])):\n",
    "            index = test_weights_abs.index(test_weights_sorted[i])\n",
    "            print(actual_labels[index], ': %.2f' % test_weights_sorted[i])\n",
    "    return test_accuracy/num_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common training helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the assembled train set without the test group\n",
    "def get_train_groups(X_groups, y_groups, num_groups, test_group):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(num_groups):\n",
    "        if (i != test_group):\n",
    "            if len(X_train) == 0:\n",
    "                X_train = np.array(X_groups[i])\n",
    "                y_train = np.array(y_groups[i])\n",
    "            else:\n",
    "                X_train = np.concatenate((X_train, X_groups[i]))\n",
    "                y_train = np.concatenate((y_train, y_groups[i]))\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_v2( X, y, alpha, iters, theta=None):\n",
    "    m,n = X.shape\n",
    "    if theta is None:\n",
    "        theta = np.ones(n)  \n",
    "    J_history = np.zeros(iters)\n",
    "\n",
    "    for i in range(0, iters):       \n",
    "        theta = theta - np.dot(alpha*X.T, np.dot(X, theta) - y)\n",
    "        J_history[i] = cost_function_v2(X, y, theta)    \n",
    "    return theta, J_history\n",
    "\n",
    "def cost_function_v2(X, y, theta):  \n",
    "    cost = 0\n",
    "    diff = (np.dot(X,theta)-y).T\n",
    "    diff = np.where(abs(diff) < 30, 0, diff)\n",
    "    cost = 0.5*np.dot(diff, diff)\n",
    "    return cost\n",
    "\n",
    "def normalize_v2(M):\n",
    "    norm_M = M.copy()\n",
    "    mean = np.mean(M[:, 1:].copy(), axis = 0)\n",
    "    std = np.array(np.std(M[:, 1:].copy(), axis = 0))\n",
    "    norm_M[:,1:] = np.divide(np.subtract(norm_M[:,1:], mean), std)\n",
    "    return norm_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(trues, predictions):\n",
    "    stat_zip = zip(predictions, trues)\n",
    "    \n",
    "    match = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for x in stat_zip:\n",
    "        if x[0] != x[1] and x[0] == 1:\n",
    "            fp = fp + 1\n",
    "        elif x[0] != x[1] and x[0] == 0:\n",
    "            fn = fn + 1\n",
    "        elif x[0] == x[1]:\n",
    "            match = match + 1\n",
    "            if x[0] == 1:\n",
    "                tp = tp + 1\n",
    "    \n",
    "    accuracy = match/len(trues)*100   \n",
    "    precision = 0 if (tp+fp == 0) else tp/(tp+fp)\n",
    "    recall = 0 if (tp+fn == 0) else tp/(tp+fn)\n",
    "    f1 = 0 if (precision+recall == 0) else 2*precision*recall/(precision+recall)\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_num_samples(y):\n",
    "    for i in range(5): \n",
    "        print(\"Samples of class\", i, \":\", len(np.nonzero(y == i)[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_labels():\n",
    "    return ['histological_grade', 'clinical_stage', 'tumor_grade', 'histological_type', 'tumor_stage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_weights(feature_labels, weights):\n",
    "    actual_labels = []\n",
    "\n",
    "    compressed_labels = get_comp_labels()\n",
    "    compressed_indices = []\n",
    "    for i in range(len(compressed_labels)):\n",
    "        compressed_indices.append([])\n",
    "\n",
    "    # extract actual labels\n",
    "    for i in range(len(feature_labels)):  \n",
    "        success = False\n",
    "        for j in range(len(compressed_labels)):\n",
    "            if feature_labels[i].startswith(compressed_labels[j]):\n",
    "                compressed_indices[j].append(i)\n",
    "                if compressed_labels[j] not in actual_labels:\n",
    "                    actual_labels.append(compressed_labels[j])\n",
    "                success = True\n",
    "                break\n",
    "        if not success:\n",
    "            actual_labels.append(feature_labels[i])\n",
    "    compressed_indices.sort(reverse = True)\n",
    "\n",
    "    # compress weights\n",
    "    for i in range(len(compressed_indices)):\n",
    "        if compressed_indices[i]:\n",
    "            num_elem = len(compressed_indices[i]) \n",
    "            start = compressed_indices[i][0]\n",
    "            finish = compressed_indices[i][0] + num_elem\n",
    "            weights[start : finish] = [np.mean( weights[start : finish])]\n",
    "    return actual_labels, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(cnf_matrix, interested_in):\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(interested_in))\n",
    "    plt.xticks(tick_marks)\n",
    "    plt.yticks(tick_marks)\n",
    "\n",
    "    # create heatmap\n",
    "    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lambdas(X_groups, y_groups, num_classes, interested_in, num_groups):\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    lambda_vals = 10.0 ** np.linspace(-5, 5, 11)\n",
    "    test_acc = np.zeros(lambda_vals.size)\n",
    "    train_acc = np.zeros(lambda_vals.size)\n",
    "\n",
    "    for j in range(lambda_vals.size): \n",
    "        train_accuracy = 0\n",
    "        test_accuracy = 0\n",
    "\n",
    "        for i in range(num_groups):\n",
    "            X_test = X_groups[i]\n",
    "            y_test = y_groups[i]\n",
    "            X_train, y_train = get_train_groups(X_groups, y_groups, num_groups, i)\n",
    "\n",
    "            weight_vectors, intercepts = train_one_vs_all(X_train, y_train, interested_in, num_classes, lambda_vals[j])\n",
    "            train_preds = predict_one_vs_all(X_train, weight_vectors, intercepts)\n",
    "            test_preds  = predict_one_vs_all(X_test, weight_vectors, intercepts)\n",
    "\n",
    "            train_accuracy = train_accuracy + np.mean(train_preds == y_train)*100\n",
    "            test_accuracy = test_accuracy + np.mean(test_preds == y_test)*100 \n",
    "\n",
    "        train_acc[j] = train_accuracy/num_groups\n",
    "        test_acc[j] = test_accuracy/num_groups\n",
    "\n",
    "    plt.xlabel('lambda (log10)')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xscale('log')\n",
    "    plt.plot(lambda_vals, train_acc, 'bo', linestyle='dashed')\n",
    "    plt.plot(lambda_vals, test_acc, 'go', linestyle='dashed')\n",
    "    plt.legend(('train', 'test')) #if you use this legend, make sure you plot the training data first and then the test data\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
