{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sklearn.linear_model as sk\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, log_loss\n",
    "from classifier import train_one_vs_all, predict_one_vs_all\n",
    "from logistic_regression import logistic, nll_cost_function, gradient_descent\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# 0: Complete Remission/Response\n",
    "# 1: Stable Disease\n",
    "# 2: Progressive Disease\n",
    "# 3: Partial Remission/Response\n",
    "# 4: Persistent Disease\n",
    "OUTCOMES_NUM = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vital status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vital_status_train(X, y, alpha, iters, cv_groups, feature_labels, train_x_times = 1, full_output = True):\n",
    "    \"\"\"\n",
    "    Trains the models to predict the vital status of a patient by the end of cancer treatment.\n",
    "\n",
    "    :param X: The input matrix (ndarray)\n",
    "    :param y: The output vector (ndarray)\n",
    "    :param alpha: The gradient descent coefficient (float)\n",
    "    :param iters: The number of gradient descent iterations (int)\n",
    "    :param cv_groups: The total number of cross-validation groups (int)\n",
    "    :param feature_labels: The list of feature labels (list)\n",
    "    :param train_x_times: The number of different batches to train the models on (int)\n",
    "    :param full_output: True for the full output (bool)\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Display only the average accuracies\n",
    "    if not full_output:\n",
    "        class_accuracy = train_accuracy = test_accuracy = bayes_accuracy = 0\n",
    "        \n",
    "        # Accumulate accuracy statistics for each training batch\n",
    "        for i in range(train_x_times):              \n",
    "            # Get training data and train the models\n",
    "            X_groups, y_groups, X_groups_full, y_groups_full = get_vital_status_data(X, y, cv_groups)\n",
    "            class_acc = vital_status_in_class(X_groups, y_groups, alpha, iters, cv_groups, full_output)\n",
    "            tr_acc, ts_acc, bs_acc = vital_status_scikit(X_groups, y_groups, X_groups_full, y_groups_full, \\\n",
    "                                                         cv_groups, feature_labels, full_output)               \n",
    "            class_accuracy = class_accuracy + class_acc\n",
    "            train_accuracy = train_accuracy + tr_acc\n",
    "            test_accuracy = test_accuracy + ts_acc\n",
    "            bayes_accuracy = bayes_accuracy + bs_acc\n",
    "        print(\"Logistic Regression (in class):\")\n",
    "        print(\"Average test accuracy: %.2f\" % (class_accuracy/train_x_times),\"%\")\n",
    "        print(\"Logistic Regression (scikit):\")      \n",
    "        print(\"Average train accuracy: %.2f\" % (train_accuracy/train_x_times),\"%\")\n",
    "        print(\"Average test accuracy: %.2f\" % (test_accuracy/train_x_times),\"%\")\n",
    "        print(\"Bayes (scikit):\")\n",
    "        print(\"Average test accuracy: %.2f\" % (bayes_accuracy/train_x_times),\"%\") \n",
    "    \n",
    "    # Display the full statistics and graphics\n",
    "    else:\n",
    "        for i in range(train_x_times):             \n",
    "            # Get training data and train the models\n",
    "            X_groups, y_groups, X_groups_full, y_groups_full = get_vital_status_data(X, y, cv_groups)\n",
    "            print(\"Logistic Regression (in class):\")\n",
    "            vital_status_in_class(X_groups, y_groups, alpha, iters, cv_groups, full_output)\n",
    "            print(\"Logistic Regression (scikit):\") \n",
    "            vital_status_scikit(X_groups, y_groups, X_groups_full, y_groups_full, \\\n",
    "                                cv_groups, feature_labels, full_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vital_status_data(X, y, cv_groups):  \n",
    "    \"\"\"\n",
    "    Returns the cross-validation groups for the vital status training.\n",
    "\n",
    "    :param X: The input matrix (ndarray)\n",
    "    :param y: The output vector (ndarray)\n",
    "    :param cv_groups: The total number of cross-validation groups (int)\n",
    "\n",
    "    :return X_groups: Reduced cross-validation groups for X (list)\n",
    "    :return y_groups: Reduced cross-validation groups for y (list)\n",
    "    :return X_groups_full: Cross-validation groups for X that include all cases (list)\n",
    "    :return y_groups_full: Cross-validation groups for y that include all cases (list)\n",
    "        \n",
    "    \"\"\"    \n",
    "    # Separate and shuffle all cases\n",
    "    dead_full = np.nonzero(y == 0)[0]\n",
    "    alive_full = np.nonzero(y == 1)[0]\n",
    "    np.random.shuffle(dead_full)\n",
    "    np.random.shuffle(alive_full)\n",
    "    \n",
    "    # Equate the number of cases of different types for in-class model\n",
    "    num_cases = min(len(dead_full), len(alive_full))\n",
    "    dead = dead_full[0:num_cases]\n",
    "    alive = alive_full[0:num_cases]\n",
    "    \n",
    "    # Split each type into equal groups\n",
    "    dead_full_groups = np.array_split(dead_full, cv_groups)\n",
    "    alive_full_groups = np.array_split(alive_full, cv_groups)\n",
    "    dead_groups = np.array_split(dead, cv_groups)\n",
    "    alive_groups = np.array_split(alive, cv_groups)\n",
    "    \n",
    "    # Accumulate the groups in lists\n",
    "    subsets = []\n",
    "    subsets_full = []\n",
    "    for i in range(cv_groups):\n",
    "        subsets_full.append(np.concatenate((dead_full_groups[i], alive_full_groups[i])))\n",
    "        np.random.shuffle(subsets_full[i]) \n",
    "        subsets.append(np.concatenate((dead_groups[i], alive_groups[i])))      \n",
    "        np.random.shuffle(subsets[i])\n",
    "\n",
    "    # Form the cross-validation groups using all possible types\n",
    "    X_groups = []\n",
    "    y_groups = []\n",
    "    X_groups_full = []\n",
    "    y_groups_full = []\n",
    "    for i in range(cv_groups):\n",
    "        X_groups_full.append(X[subsets_full[i],:])\n",
    "        y_groups_full.append(y[subsets_full[i]])      \n",
    "        X_groups.append(X[subsets[i],:])\n",
    "        y_groups.append(y[subsets[i]])\n",
    "        \n",
    "    return X_groups, y_groups, X_groups_full, y_groups_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vital_status_in_class(X_groups, y_groups, alpha, iters, cv_groups, full_output = True):\n",
    "    \"\"\"\n",
    "    Trains the in-class model to predict the vital status.\n",
    "\n",
    "    :param X_groups: The cross-validation groups for X (list)\n",
    "    :param y_groups: The cross-validation groups for y (list) \n",
    "    :param alpha: The gradient descent coefficient (float)\n",
    "    :param iters: The number of gradient descent iterations (int)\n",
    "    :param cv_groups: The total number of cross-validation groups (int)\n",
    "    :param full_output: True for the full output (bool)\n",
    "\n",
    "    :return: Average LR test accuracy (float)\n",
    "        \n",
    "    \"\"\"\n",
    "    # Define parameters for the training\n",
    "    theta = np.ones(X_groups[0].shape[1])\n",
    "    train_accuracy = test_accuracy = train_f1_score = test_f1_score = 0\n",
    "    gradient_cost = 0\n",
    "    test_confusion = np.zeros(shape=(2, 2))\n",
    "\n",
    "    for i in range(cv_groups):\n",
    "        # Get train and test X and y batches\n",
    "        X_train, y_train = get_train_groups(X_groups, y_groups, cv_groups, i)\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        \n",
    "        # Train the in-class model\n",
    "        theta, J = gradient_descent(X_train, y_train, theta, alpha, iters)\n",
    "        gradient_cost = gradient_cost + J[-1]\n",
    "               \n",
    "        # Get train and test predictions\n",
    "        sort_predictions = lambda x: 0 if x < 0.5 else 1\n",
    "        convert_to_log = np.vectorize(sort_predictions)\n",
    "        train_preds = logistic(np.dot(X_train, theta))\n",
    "        train_preds = convert_to_log(train_preds)\n",
    "        test_preds = logistic(np.dot(X_test, theta))\n",
    "        test_preds = convert_to_log(test_preds)\n",
    "        \n",
    "        # Get the statistics\n",
    "        accuracy, f1 = get_stats(y_train, train_preds)\n",
    "        train_accuracy = train_accuracy + accuracy\n",
    "        train_f1_score = train_f1_score + f1\n",
    "        accuracy, f1 = get_stats(y_test, test_preds)\n",
    "        test_accuracy = test_accuracy + accuracy\n",
    "        test_f1_score = test_f1_score + f1\n",
    "        test_confusion = test_confusion + confusion_matrix(y_test, test_preds)\n",
    "    \n",
    "    # Display the full statistics and graphics\n",
    "    if full_output:\n",
    "        print(\"Average train final cost: %.2f\" % (gradient_cost/cv_groups))\n",
    "        print(\"Average train accuracy: %.2f\" % (train_accuracy/cv_groups),\"%\")\n",
    "        print(\"Average train F1 score: %.2f\" % (train_f1_score/cv_groups))\n",
    "        print(\"Average test accuracy: %.2f\" % (test_accuracy/cv_groups),\"%\")\n",
    "        print(\"Average test F1 score: %.2f\" % (test_f1_score/cv_groups))\n",
    "        create_heatmap((test_confusion/cv_groups).astype('int'), [0, 1])\n",
    "        \n",
    "    return test_accuracy/cv_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vital_status_scikit(X_groups, y_groups, X_groups_full, y_groups_full, cv_groups, feature_labels, full_output = True):\n",
    "    \"\"\"\n",
    "    Trains the scikit model to predict the vital status.\n",
    "\n",
    "    :param X_groups: The cross-validation groups for reduced X (list)\n",
    "    :param y_groups: The cross-validation groups for reduced y (list) \n",
    "    :param X_groups_full: The cross-validation groups for full X (list)\n",
    "    :param y_groups_full: The cross-validation groups for full y (list) \n",
    "    :param cv_groups: The total number of cross-validation groups (int)\n",
    "    :param feature_labels: The list of feature labels (list)\n",
    "    :param full_output: True for the full output (bool)\n",
    "    \n",
    "    :return: Average LR train accuracy (float)\n",
    "    :return: Average LR test accuracy (float)\n",
    "    :return: Average Bayes test accuracy (float)\n",
    "        \n",
    "    \"\"\"    \n",
    "    # Define parameters for the training\n",
    "    train_accuracy = test_accuracy = bayes_accuracy = 0\n",
    "    train_cost = test_cost = 0\n",
    "    train_f1_score = test_f1_score = bayes_f1_score = 0\n",
    "    test_weights = np.zeros(X_groups[0].shape[1] - 1)\n",
    "    test_confusion = np.zeros(shape=(2, 2)) \n",
    "\n",
    "    for i in range(cv_groups):\n",
    "        # Get train and test X and y batches\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i] \n",
    "        X_train, y_train = get_train_groups(X_groups, y_groups, cv_groups, i)\n",
    "        X_test_full = X_groups_full[i]\n",
    "        y_test_full = y_groups_full[i] \n",
    "        X_train_full, y_train_full = get_train_groups(X_groups_full, y_groups_full, cv_groups, i)\n",
    "        \n",
    "        # Train the Logistic Regression and Gaussian Bayes models\n",
    "        lr = sk.LogisticRegression(solver='liblinear', class_weight = 'balanced')\n",
    "        lr.fit(X_train_full[:,1:], y_train_full)\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(X_train[:,1:], y_train)\n",
    "        \n",
    "        # Get train and test predictions\n",
    "        train_preds = lr.predict(X_train_full[:,1:])\n",
    "        test_preds = lr.predict(X_test_full[:,1:])\n",
    "        gnb_preds = gnb.predict(X_test[:,1:])\n",
    "        \n",
    "        # Get the statistics\n",
    "        bayes_accuracy = bayes_accuracy + accuracy_score(y_test, gnb_preds)*100\n",
    "        bayes_f1_score = bayes_f1_score + f1_score(y_test, gnb_preds)       \n",
    "        train_accuracy = train_accuracy + accuracy_score(y_train_full, train_preds)*100\n",
    "        train_f1_score = train_f1_score + f1_score(y_train_full, train_preds)\n",
    "        train_cost = train_cost + log_loss(y_train_full, train_preds)        \n",
    "        test_accuracy = test_accuracy + accuracy_score(y_test_full, test_preds)*100\n",
    "        test_f1_score = test_f1_score + f1_score(y_test_full, test_preds)\n",
    "        test_cost = test_cost + log_loss(y_test_full, test_preds)        \n",
    "        test_weights = np.add(test_weights, lr.coef_[0])\n",
    "        test_confusion = test_confusion + confusion_matrix(y_test_full, test_preds)\n",
    "\n",
    "    # Get the labels with the highest average absolute weights\n",
    "    actual_labels, actual_weights = get_actual_weights(feature_labels[1:], list(test_weights))    \n",
    "    test_weights_abs = list(np.abs(actual_weights))\n",
    "    test_weights_sorted = test_weights_abs.copy()\n",
    "    test_weights_sorted.sort(reverse = True)\n",
    "    \n",
    "    # Display the full statistics and graphics\n",
    "    if full_output:\n",
    "        print(\"Average train final cost: %.2f\" % (train_cost/cv_groups))\n",
    "        print(\"Average train accuracy: %.2f\" % (train_accuracy/cv_groups),\"%\")\n",
    "        print(\"Average train F1 score: %.2f\" % (train_f1_score/cv_groups))\n",
    "        print(\"Average test final cost: %.2f\" % (test_cost/cv_groups))\n",
    "        print(\"Average test accuracy: %.2f\" % (test_accuracy/cv_groups),\"%\")\n",
    "        print(\"Average test F1 score: %.2f\" % (test_f1_score/cv_groups))\n",
    "        print(\"Average Gaussian Naive Bayes accuracy: %.2f\" % (bayes_accuracy/cv_groups),\"%\")\n",
    "        print(\"Average Gaussian Naive Bayes F1 score: %.2f\" % (bayes_f1_score/cv_groups))\n",
    "        create_heatmap((test_confusion/cv_groups).astype('int'), [0, 1])\n",
    "        print()\n",
    "        print(\"Highest average weights (absolute values):\")\n",
    "        for i in range(len(test_weights_sorted[:3])):\n",
    "            index = test_weights_abs.index(test_weights_sorted[i])\n",
    "            print(actual_labels[index], ': %.2f' % test_weights_sorted[i])\n",
    "            \n",
    "    return train_accuracy/cv_groups, test_accuracy/cv_groups, bayes_accuracy/cv_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Death days to**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def death_days_to_train(X, y, alpha, iters, num_groups):\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    final_cost = 0\n",
    "    \n",
    "    # Get cross-validation grouped X and y data\n",
    "    X_groups, y_groups = get_death_days_to_data(X, y, num_groups)\n",
    "\n",
    "    # Perform k-fold cross-validation and record the results \n",
    "    for i in range(num_groups):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        X_train, y_train = get_train_groups(X_groups, y_groups, num_groups, i)\n",
    "\n",
    "        # Scikit model training\n",
    "        lr = sk.LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        train_preds = lr.predict(X_train)\n",
    "        test_preds = lr.predict(X_test)\n",
    "\n",
    "        # In-class, only cost\n",
    "        theta_vec_equations, cost = gradient_descent_v2(normalize_v2(X_train), y_train, alpha, iters)   \n",
    "        final_cost = final_cost + cost[-1]\n",
    "\n",
    "        # Calculate the train accuracy\n",
    "        match = 0\n",
    "        for i in range(len(y_train)):\n",
    "            if  train_preds[i] <= y_train[i] + 180 and train_preds[i] >= y_train[i] - 180:\n",
    "                match += 1\n",
    "        train_accuracy = train_accuracy + match/len(y_train)*100 \n",
    "\n",
    "        # Calculate the test accuracy\n",
    "        match = 0\n",
    "        for i in range(len(y_test)):\n",
    "            if  test_preds[i] <= y_test[i] + 180 and test_preds[i] >= y_test[i] - 180:\n",
    "                match += 1\n",
    "        test_accuracy = test_accuracy + match/len(y_test)*100 \n",
    "\n",
    "    print(\"In-class model:\")\n",
    "    print(\"Average final cost: %.2f\" % (final_cost/num_groups))\n",
    "    print(\"Mean in y: %.2f\" % np.mean(y))\n",
    "    print(\"Variance in y: %.2f\" % np.var(y))\n",
    "    print()\n",
    "    print(\"Scikit model:\")\n",
    "    print(\"Average train accuracy: %.2f\" % (train_accuracy/num_groups),\"%\")\n",
    "    print(\"Average test accuracy: %.2f\" % (test_accuracy/num_groups),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation groups for death_days_to\n",
    "def get_death_days_to_data(X, y, num_groups):    \n",
    "    # Get all dead cases\n",
    "    death_indices = np.arange(len(y))\n",
    "    np.random.shuffle(death_indices)\n",
    "    \n",
    "    dead_groups = np.array_split(death_indices, num_groups)\n",
    "\n",
    "    X_groups = []\n",
    "    y_groups = []\n",
    "    for i in range(num_groups):\n",
    "        X_groups.append(X[dead_groups[i],:])\n",
    "        y_groups.append(y[dead_groups[i]])\n",
    "    return X_groups, y_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_train(X, y, interested_in, lambda_val, num_groups, feature_labels, \\\n",
    "                  alpha = 0.00000001, iters = 2000, num_tests = 1, output = True):\n",
    "    if output == True:\n",
    "        for i in range(num_tests):    \n",
    "            X_groups, y_groups, num_classes = get_outcome_data(X, y, interested_in, num_groups)\n",
    "\n",
    "            print(\"In class model:\")\n",
    "            if num_classes == 2:\n",
    "                for i in range(len(y_groups)):\n",
    "                    y_groups[i] = np.where(y_groups[i] == interested_in[0], 0, 1)                    \n",
    "                print(\"Logistic regression:\")\n",
    "                vital_status_in_class(X_groups, y_groups, alpha, iters, num_groups)\n",
    "                print()\n",
    "\n",
    "            print(\"One vs all:\")  \n",
    "            outcome_in_class(X_groups, y_groups, num_classes, interested_in, lambda_val, num_groups)\n",
    "\n",
    "            print()\n",
    "            print(\"Scikit model:\")\n",
    "            outcome_scikit(X_groups, y_groups, num_classes, interested_in, num_groups, feature_labels)\n",
    "            plot_lambdas(X_groups, y_groups, num_classes, interested_in, num_groups)\n",
    "    else:\n",
    "        accuracy_regression = 0\n",
    "        accuracy_classifier = 0\n",
    "        accuracy_scikit = 0\n",
    "        accuracy_bayes = 0\n",
    "        \n",
    "        for i in range(num_tests):  \n",
    "            X_groups, y_groups, num_classes = get_outcome_data(X, y, interested_in, num_groups)\n",
    "\n",
    "            if num_classes == 2:\n",
    "                for i in range(len(y_groups)):\n",
    "                    y_groups[i] = np.where(y_groups[i] == interested_in[0], 0, 1)                    \n",
    "                accuracy_regression = accuracy_regression + vital_status_in_class(X_groups, y_groups, alpha, iters, num_groups, output) \n",
    "            accuracy_classifier = accuracy_classifier + outcome_in_class(X_groups, y_groups, num_classes, interested_in, lambda_val, num_groups, output)\n",
    "            temp1, temp2 = outcome_scikit(X_groups, y_groups, num_classes, interested_in, num_groups, feature_labels, output)\n",
    "            accuracy_scikit = accuracy_scikit + temp1       \n",
    "            accuracy_bayes = accuracy_bayes + temp2    \n",
    "                \n",
    "        if num_classes == 2:\n",
    "            print(\"Logistic regression:\")\n",
    "            print(\"Average test accuracy: %.2f\" % (accuracy_regression/num_tests),\"%\")\n",
    "        print(\"One vs all model:\")\n",
    "        print(\"Average test accuracy: %.2f\" % (accuracy_classifier/num_tests),\"%\")\n",
    "        print(\"Scikit model:\")\n",
    "        print(\"Average test accuracy: %.2f\" % (accuracy_scikit/num_tests),\"%\")\n",
    "        print(\"Bayes model:\")\n",
    "        print(\"Average test accuracy: %.2f\" % (accuracy_bayes/num_tests),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outcome_data(X, y, interested_in, num_groups):    \n",
    "    # Get the number of samples to take from each class\n",
    "    lengths = []\n",
    "    data = []\n",
    "    for i in range(OUTCOMES_NUM): \n",
    "        if i in interested_in:\n",
    "            data.append(np.nonzero(y == i)[0])\n",
    "            lengths.append(len(data[-1]))    \n",
    "    num_cases = np.amin(lengths)\n",
    "    num_classes = len(lengths)\n",
    "    \n",
    "    \n",
    "    # Truncate to balance\n",
    "    for i in range(num_classes):\n",
    "        np.random.shuffle(data[i])\n",
    "        data[i] = data[i][0:num_cases]\n",
    "    \n",
    "    # Break into balanced groups\n",
    "    batch_groups = []\n",
    "    for i in range(num_classes):\n",
    "        batch_groups.append(np.array_split(data[i], num_groups))\n",
    "\n",
    "    # Put batches together \n",
    "    outcome_groups = []\n",
    "    for i in range(num_groups): #0-4\n",
    "        batch = []\n",
    "        for j in range(num_classes): #0-2\n",
    "            batch = np.concatenate([batch, batch_groups[j][i]])\n",
    "        np.random.shuffle(batch)\n",
    "        outcome_groups.append(batch.astype('int'))\n",
    "    \n",
    "    X_groups = []\n",
    "    y_groups = []\n",
    "    for i in range(num_groups):\n",
    "        X_groups.append(X[outcome_groups[i],:])\n",
    "        y_groups.append(y[outcome_groups[i]])\n",
    "        \n",
    "    return X_groups, y_groups, X_groups_full, y_groups_full, num_classes\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get the number of samples to take from each class\n",
    "    lengths = []\n",
    "    data = []\n",
    "    for i in range(5): \n",
    "        if i in interested_in:\n",
    "            data.append(np.nonzero(y == i)[0])\n",
    "            lengths.append(len(data[-1]))    \n",
    "    num_cases = np.amin(lengths)\n",
    "    num_classes = len(lengths)\n",
    "    \n",
    "    \n",
    "    # Truncate to balance\n",
    "    for i in range(num_classes):\n",
    "        np.random.shuffle(data[i])\n",
    "        data[i] = data[i][0:num_cases]\n",
    "    \n",
    "    # Break into balanced groups\n",
    "    batch_groups = []\n",
    "    for i in range(num_classes):\n",
    "        batch_groups.append(np.array_split(data[i], num_groups))\n",
    "\n",
    "    # Put batches together \n",
    "    outcome_groups = []\n",
    "    for i in range(num_groups): #0-4\n",
    "        batch = []\n",
    "        for j in range(num_classes): #0-2\n",
    "            batch = np.concatenate([batch, batch_groups[j][i]])\n",
    "        np.random.shuffle(batch)\n",
    "        outcome_groups.append(batch.astype('int'))\n",
    "    \n",
    "    X_groups = []\n",
    "    y_groups = []\n",
    "    for i in range(num_groups):\n",
    "        X_groups.append(X[outcome_groups[i],:])\n",
    "        y_groups.append(y[outcome_groups[i]])\n",
    "    return X_groups, y_groups, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_in_class(X_groups, y_groups, num_classes, interested_in, lambda_val, num_groups, output = True):\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    train_f1_score = 0\n",
    "    test_f1_score = 0 \n",
    "    test_confusion = np.zeros(shape=(len(interested_in), len(interested_in)))\n",
    "    \n",
    "    target_names = []\n",
    "    for i in interested_in:\n",
    "        target_names.append('class ' + str(i))\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        X_train, y_train = get_train_groups(X_groups, y_groups, num_groups, i)\n",
    "        \n",
    "        weight_vectors, intercepts = train_one_vs_all(X_train[:,1:], y_train, interested_in, num_classes, lambda_val)\n",
    "        train_preds = predict_one_vs_all(X_train[:,1:], weight_vectors, intercepts)\n",
    "        test_preds  = predict_one_vs_all(X_test[:,1:],  weight_vectors, intercepts)\n",
    "\n",
    "        accuracy, f1 = get_stats(y_train, train_preds)\n",
    "        train_accuracy = train_accuracy + accuracy\n",
    "        train_f1_score = train_f1_score + f1\n",
    "        \n",
    "        accuracy, f1 = get_stats(y_test, test_preds)\n",
    "        test_accuracy = test_accuracy + accuracy\n",
    "        test_f1_score = test_f1_score + f1\n",
    "        test_confusion = test_confusion + confusion_matrix(list(y_test), list(test_preds))\n",
    "    if output:\n",
    "        print(\"Average train accuracy: %.2f\" % (train_accuracy/num_groups),\"%\")\n",
    "        print(\"Average train F1 score: %.2f\" % (train_f1_score/num_groups))\n",
    "\n",
    "        print(\"Average test accuracy: %.2f\" % (test_accuracy/num_groups),\"%\")\n",
    "        print(\"Average test F1 score: %.2f\" % (test_f1_score/num_groups))   \n",
    "        create_heatmap((test_confusion/num_groups).astype('int'), interested_in)\n",
    "    return test_accuracy/num_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug 1.1: confusion matrix is not exactly what we need it to be. \n",
    "# Bug 1.2: test_confusion is not initialized right\n",
    "\n",
    "def outcome_scikit(X_groups, y_groups, num_classes, interested_in, num_groups, feature_labels, output = True):\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "    train_f1_score = 0\n",
    "    test_f1_score = 0\n",
    "    bayes_f1_score = 0\n",
    "    test_weights = np.zeros(X_groups[0].shape[1] - 1)\n",
    "    test_confusion = np.zeros(shape=(len(interested_in), len(interested_in)))\n",
    "    test_bayes = 0\n",
    "    \n",
    "    target_names = []\n",
    "    for i in interested_in:\n",
    "        target_names.append('class ' + str(i))\n",
    "    \n",
    "    for i in range(num_groups):\n",
    "        X_test = X_groups[i]\n",
    "        y_test = y_groups[i]\n",
    "        X_train, y_train = get_train_groups(X_groups, y_groups, num_groups, i)\n",
    "        \n",
    "        lr = sk.LogisticRegression()\n",
    "        lr.fit(X_train[:,1:], y_train.astype('int'))\n",
    "        train_preds = lr.predict(X_train[:,1:])\n",
    "        test_preds = lr.predict(X_test[:,1:])\n",
    "        \n",
    "#         lr1 = sk.LogisticRegression(class_weight = 'balanced')\n",
    "#         lr1.fit(X_train[:,1:], y_train.astype('int'))\n",
    "#         train_preds1 = lr1.predict(X_train[:,1:])\n",
    "#         test_preds1 = lr1.predict(X_test[:,1:])\n",
    "        \n",
    "#         print(\"train:\", np.mean(train_preds == y_test)*100, \"to\", np.mean(train_preds1 == y_test)*100)\n",
    "#         print(\"test:\", np.mean(test_preds == y_test)*100, \"to\", np.mean(test_preds1 == y_test)*100)      \n",
    "        \n",
    "        # Naive Bayes model\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(X_train[:,1:], y_train.astype('int'))       \n",
    "        gnb_preds = gnb.predict(X_test[:,1:])\n",
    "        \n",
    "        train_accuracy = train_accuracy + np.mean(train_preds == y_train)*100\n",
    "        test_accuracy = test_accuracy + np.mean(test_preds == y_test)*100       \n",
    "        test_bayes = test_bayes + accuracy_score(list(y_test), list(gnb_preds))*100\n",
    "        train_class_report = classification_report(list(y_train), list(train_preds), \\\n",
    "                                              target_names = target_names, output_dict=True)\n",
    "        test_class_report = classification_report(list(y_test), list(test_preds), \\\n",
    "                                              target_names = target_names, output_dict=True)     \n",
    "        bayes_class_report = classification_report(list(y_test), list(gnb_preds), \\\n",
    "                                                   target_names = target_names, output_dict=True)\n",
    "\n",
    "        # Calculate average test f1-score\n",
    "        train_f1_score = train_f1_score + train_class_report['weighted avg']['f1-score']\n",
    "        test_f1_score = test_f1_score + test_class_report['weighted avg']['f1-score']\n",
    "        bayes_f1_score = bayes_f1_score + bayes_class_report['weighted avg']['f1-score']\n",
    "        test_weights = np.add(test_weights, lr.coef_[0])\n",
    "        test_confusion = test_confusion + confusion_matrix(list(y_test), list(test_preds))\n",
    "    \n",
    "    actual_labels, actual_weights = get_actual_weights(feature_labels[1:], list(test_weights))   \n",
    "    test_weights_abs = list(np.abs(actual_weights))\n",
    "    test_weights_sorted = test_weights_abs.copy()\n",
    "    test_weights_sorted.sort(reverse = True)\n",
    "    \n",
    "    if output:\n",
    "        print(\"Average train accuracy: %.2f\" % (train_accuracy/num_groups),\"%\")\n",
    "        print(\"Average train F1 score: %.2f\" % (train_f1_score/num_groups))\n",
    "        print(\"Average test accuracy: %.2f\" % (test_accuracy/num_groups),\"%\")\n",
    "        print(\"Average test F1 score: %.2f\" % (test_f1_score/num_groups))\n",
    "        print(\"Average Gaussian Naive Bayes accuracy: %.2f\" % (test_bayes/num_groups),\"%\")\n",
    "        print(\"Average Gaussian Naive Bayes F1 score: %.2f\" % (bayes_f1_score/num_groups))\n",
    "        create_heatmap((test_confusion/num_groups).astype('int'), interested_in)\n",
    "        print()\n",
    "        print(\"Highest average weights (absolute values!):\")\n",
    "        for i in range(len(test_weights_sorted[:3])):\n",
    "            index = test_weights_abs.index(test_weights_sorted[i])\n",
    "            print(actual_labels[index], ': %.2f' % test_weights_sorted[i])\n",
    "    return test_accuracy/num_groups, test_bayes/num_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common training helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the assembled train set without the test group\n",
    "def get_train_groups(X_groups, y_groups, num_groups, test_group):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(num_groups):\n",
    "        if (i != test_group):\n",
    "            if len(X_train) == 0:\n",
    "                X_train = np.array(X_groups[i])\n",
    "                y_train = np.array(y_groups[i])\n",
    "            else:\n",
    "                X_train = np.concatenate((X_train, X_groups[i]))\n",
    "                y_train = np.concatenate((y_train, y_groups[i]))\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_v2( X, y, alpha, iters, theta=None):\n",
    "    m,n = X.shape\n",
    "    if theta is None:\n",
    "        theta = np.ones(n)  \n",
    "    J_history = np.zeros(iters)\n",
    "\n",
    "    for i in range(0, iters):       \n",
    "        theta = theta - np.dot(alpha*X.T, np.dot(X, theta) - y)\n",
    "        J_history[i] = cost_function_v2(X, y, theta)    \n",
    "    return theta, J_history\n",
    "\n",
    "def cost_function_v2(X, y, theta):  \n",
    "    cost = 0\n",
    "    diff = (np.dot(X,theta)-y).T\n",
    "    diff = np.where(abs(diff) < 30, 0, diff)\n",
    "    cost = 0.5*np.dot(diff, diff)\n",
    "    return cost\n",
    "\n",
    "def normalize_v2(M):\n",
    "    norm_M = M.copy()\n",
    "    mean = np.mean(M[:, 1:].copy(), axis = 0)\n",
    "    std = np.array(np.std(M[:, 1:].copy(), axis = 0))\n",
    "    norm_M[:,1:] = np.divide(np.subtract(norm_M[:,1:], mean), std)\n",
    "    return norm_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(trues, predictions):\n",
    "    stat_zip = zip(predictions, trues)\n",
    "    \n",
    "    match = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for x in stat_zip:\n",
    "        if x[0] != x[1] and x[0] == 1:\n",
    "            fp = fp + 1\n",
    "        elif x[0] != x[1] and x[0] == 0:\n",
    "            fn = fn + 1\n",
    "        elif x[0] == x[1]:\n",
    "            match = match + 1\n",
    "            if x[0] == 1:\n",
    "                tp = tp + 1\n",
    "    \n",
    "    accuracy = match/len(trues)*100   \n",
    "    precision = 0 if (tp+fp == 0) else tp/(tp+fp)\n",
    "    recall = 0 if (tp+fn == 0) else tp/(tp+fn)\n",
    "    f1 = 0 if (precision+recall == 0) else 2*precision*recall/(precision+recall)\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_num_samples(y):\n",
    "    for i in range(OUTCOMES_NUM): \n",
    "        print(\"Samples of class\", i, \":\", len(np.nonzero(y == i)[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comp_labels():\n",
    "    return ['histological_grade', 'clinical_stage', 'tumor_grade', 'histological_type', 'tumor_stage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_weights(feature_labels, weights):\n",
    "    actual_labels = []\n",
    "\n",
    "    compressed_labels = get_comp_labels()\n",
    "    compressed_indices = []\n",
    "    for i in range(len(compressed_labels)):\n",
    "        compressed_indices.append([])\n",
    "\n",
    "    # extract actual labels\n",
    "    for i in range(len(feature_labels)):  \n",
    "        success = False\n",
    "        for j in range(len(compressed_labels)):\n",
    "            if feature_labels[i].startswith(compressed_labels[j]):\n",
    "                compressed_indices[j].append(i)\n",
    "                if compressed_labels[j] not in actual_labels:\n",
    "                    actual_labels.append(compressed_labels[j])\n",
    "                success = True\n",
    "                break\n",
    "        if not success:\n",
    "            actual_labels.append(feature_labels[i])\n",
    "    compressed_indices.sort(reverse = True)\n",
    "\n",
    "    # compress weights\n",
    "    for i in range(len(compressed_indices)):\n",
    "        if compressed_indices[i]:\n",
    "            num_elem = len(compressed_indices[i]) \n",
    "            start = compressed_indices[i][0]\n",
    "            finish = compressed_indices[i][0] + num_elem\n",
    "            weights[start : finish] = [np.mean( weights[start : finish])]\n",
    "    return actual_labels, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(cnf_matrix, interested_in):\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(interested_in))\n",
    "    plt.xticks(tick_marks)\n",
    "    plt.yticks(tick_marks)\n",
    "\n",
    "    # create heatmap\n",
    "    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lambdas(X_groups, y_groups, num_classes, interested_in, num_groups):\n",
    "    train_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    lambda_vals = 10.0 ** np.linspace(-5, 5, 11)\n",
    "    test_acc = np.zeros(lambda_vals.size)\n",
    "    train_acc = np.zeros(lambda_vals.size)\n",
    "\n",
    "    for j in range(lambda_vals.size): \n",
    "        train_accuracy = 0\n",
    "        test_accuracy = 0\n",
    "\n",
    "        for i in range(num_groups):\n",
    "            X_test = X_groups[i]\n",
    "            y_test = y_groups[i]\n",
    "            X_train, y_train = get_train_groups(X_groups, y_groups, num_groups, i)\n",
    "\n",
    "            weight_vectors, intercepts = train_one_vs_all(X_train, y_train, interested_in, num_classes, lambda_vals[j])\n",
    "            train_preds = predict_one_vs_all(X_train, weight_vectors, intercepts)\n",
    "            test_preds  = predict_one_vs_all(X_test, weight_vectors, intercepts)\n",
    "\n",
    "            train_accuracy = train_accuracy + np.mean(train_preds == y_train)*100\n",
    "            test_accuracy = test_accuracy + np.mean(test_preds == y_test)*100 \n",
    "\n",
    "        train_acc[j] = train_accuracy/num_groups\n",
    "        test_acc[j] = test_accuracy/num_groups\n",
    "\n",
    "    plt.xlabel('lambda (log10)')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xscale('log')\n",
    "    plt.plot(lambda_vals, train_acc, 'bo', linestyle='dashed')\n",
    "    plt.plot(lambda_vals, test_acc, 'go', linestyle='dashed')\n",
    "    plt.legend(('train', 'test')) \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
